{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PassGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from data import load_dataset, dump_txt_to_pickle, load_data_from_pickle, dataloader, translate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name(), \"|\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/rockyou_processed.txt\"\n",
    "dataset_name = \"rockyou\"\n",
    "\n",
    "# filtered_lines, charmap, inv_charmap = load_dataset(path)\n",
    "# dump_txt_to_pickle(path, dataset_name, test_size=0.1)\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "train_lines, charmap, inv_charmap = load_data_from_pickle(dataset_name)\n",
    "print(datetime.datetime.now() - t)\n",
    "\n",
    "# train = dataloader(train_lines, 8)\n",
    "# translate(next(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop unrolled from the function in `training.py`\n",
    "\n",
    "Function parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training_loop(lines, charmap, inv_charmap, dataloader, args):\n",
    "lines = train_lines\n",
    "dataloader = dataloader #function from data.py\n",
    "\n",
    "args = {}\n",
    "args['lambda_'] = 10\n",
    "args['n_critic_iters_per_generator_iter'] = 10\n",
    "args['batch_size'] = 128\n",
    "args['lr'] = 1e-4\n",
    "args['adam_beta1'] = 0.5\n",
    "args['adam_beta2'] = 0.9\n",
    "args['iterations'] = 199000\n",
    "args['continue_training'] = False\n",
    "args['netG_checkpoint'] = None\n",
    "args['netD_checkpoint'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing arguments, model, optimizers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = args['lambda_']\n",
    "n_critic_iters_per_generator_iter = args['n_critic_iters_per_generator_iter']\n",
    "batch_size = args['batch_size']\n",
    "lr = args['lr']\n",
    "adam_beta1 = args['adam_beta1']\n",
    "adam_beta2 = args['adam_beta2']\n",
    "iterations = args['iterations']\n",
    "continue_training = args['continue_training']\n",
    "netG_checkpoint = args['netG_checkpoint']\n",
    "netD_checkpoint = args['netD_checkpoint']\n",
    "\n",
    "# Start actual training loop\n",
    "\n",
    "netG = Generator(charmap).to(device=device)\n",
    "netD = Discriminator(charmap).to(device=device)\n",
    "\n",
    "train = dataloader(lines, batch_size)\n",
    "\n",
    "if continue_training: #if continuing from checkpoint\n",
    "    netG.load_state_dict(torch.load(netG_checkpoint))\n",
    "    netD.load_state_dict(torch.load(netD_checkpoint))\n",
    "    start_iter = int(netG_checkpoint.split(\":\")[0].split(\"-\")[-1][:-2])\n",
    "    for _ in range(start_iter): #look up better way to do this\n",
    "        next(train)\n",
    "        pass\n",
    "    print(f\"Model loaded, starting at {start_iter}...\")\n",
    "else: \n",
    "    start_iter = 1\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=lr, betas=(adam_beta1, adam_beta2))\n",
    "optimD = optim.Adam(netD.parameters(), lr=lr, betas=(adam_beta1, adam_beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(start_iter, iterations + 1):\n",
    "    for p in netD.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "\n",
    "    for i in range(n_critic_iters_per_generator_iter):\n",
    "        real_inputs_discrete = next(train)\n",
    "        real_data = F.one_hot(real_inputs_discrete, num_classes=len(charmap)).float() #x\n",
    "        latent_variable = torch.randn(batch_size, 128).to(device=device) #z\n",
    "        alpha = torch.rand(batch_size, 1, 1).to(device=device) #epsilon\n",
    "\n",
    "        fake_data = netG(latent_variable) #x_tilde\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data) #x_hat\n",
    "        interpolates = interpolates.clone().detach().requires_grad_(True) #x_hat\n",
    "        disc_interpolates = netD(interpolates) #D_w(x_hat)\n",
    "        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates, #grad D_w(x_hat)\n",
    "                        grad_outputs=torch.ones(disc_interpolates.size()).to(device=device),\n",
    "                        create_graph=True, retain_graph=True, only_inputs=True)[0] #doesn't populate grad attributes\n",
    "\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "\n",
    "        disc_real = netD(real_data).mean() #D_w(x)\n",
    "        disc_fake = netD(fake_data).mean() #D_w(x_tilde)\n",
    "\n",
    "        loss = disc_fake - disc_real + gradient_penalty #L\n",
    "        loss.backward()\n",
    "        optimD.step()\n",
    "        netD.zero_grad()\n",
    "\n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "    netG.zero_grad()\n",
    "\n",
    "    latent_variable = torch.randn(batch_size, 128).to(device=device) #z\n",
    "    fake_data = netG(latent_variable)    \n",
    "    G = -netD(fake_data).mean()\n",
    "    G.backward()\n",
    "    optimG.step()\n",
    "\n",
    "    if iteration % 1000 == 0 or iteration == 1:\n",
    "        print(f\"iterations {iteration}\")\n",
    "        real_translation = translate(real_inputs_discrete[:10].cpu().numpy(), inv_charmap)\n",
    "        fake_translation = translate(fake_data[:10].detach().cpu().numpy().argmax(axis=2), inv_charmap)\n",
    "        print(f\"\\tFake: {fake_translation}\\n\\tReal: {real_translation}\")\n",
    "        time = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=-5))).strftime(\"%I:%M:%S%p_%m-%d-%y\")\n",
    "        torch.save(netG.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netG-{iteration}{time}\")\n",
    "        torch.save(netD.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netD-{iteration}{time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
