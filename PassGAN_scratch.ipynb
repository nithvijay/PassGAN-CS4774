{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used for scratch and testing purposes. `PassGAN.ipynb` has more updated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name(), \"|\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(max_vocab_size=2048):\n",
    "    path = \"Data/rockyou_processed.txt\"\n",
    "    with open(path, 'r') as f:\n",
    "        lines = [line for line in f]\n",
    "\n",
    "    np.random.shuffle(lines)\n",
    "\n",
    "    import collections\n",
    "    counts = collections.Counter(char for line in lines for char in line if char != \"\\n\")\n",
    "\n",
    "    charmap = {'unk':0}\n",
    "    inv_charmap = ['unk']\n",
    "\n",
    "    for char,count in counts.most_common(max_vocab_size-1):\n",
    "        if char not in charmap:\n",
    "            charmap[char] = len(inv_charmap)\n",
    "            inv_charmap.append(char)\n",
    "\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        filtered_line = []\n",
    "        for char in line:\n",
    "            if char in charmap:\n",
    "                filtered_line.append(char)\n",
    "            else:\n",
    "                filtered_line.append('unk')\n",
    "        filtered_lines.append(tuple(filtered_line))\n",
    "\n",
    "    print(\"loaded {} lines in dataset\".format(len(lines)))\n",
    "    return filtered_lines, charmap, inv_charmap\n",
    "\n",
    "filtered_lines, charmap, inv_charmap = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(filtered_lines, open(\"Data/rockyou.pickle\", 'wb'))\n",
    "pickle.dump(charmap, open(\"Checkpoints/rockyou_charmap.pickle\", 'wb'))\n",
    "pickle.dump(inv_charmap, open(\"Checkpoints/rockyou_inv_charmap.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = datetime.datetime.now()\n",
    "filtered_lines = pickle.load(open(\"Data/rockyou.pickle\", 'rb'))\n",
    "charmap = pickle.load(open(\"Checkpoints/rockyou_charmap.pickle\", \"rb\"))\n",
    "inv_charmap = pickle.load(open(\"Checkpoints/rockyou_inv_charmap.pickle\", \"rb\"))\n",
    "print(datetime.datetime.now() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(lines, batch_size):\n",
    "    while True:\n",
    "        np.random.shuffle(lines)\n",
    "        for i in range(len(lines) // batch_size):\n",
    "            yield torch.tensor(lines[i*batch_size:(i+1)*batch_size]).to(device=device)\n",
    "        \n",
    "def translate(passwords):\n",
    "    return [\"\".join([inv_charmap[c] for c in password]) for password in passwords]\n",
    "\n",
    "# train = dataloader(filtered_lines, 4)\n",
    "# translate(next(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual blocks take BATCH_SIZE x CHANNELS x LENGTH -> BATCH_SIZE x CHANNELS x LENGTH\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(inputs)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        outputs = inputs + x * 0.3\n",
    "        return outputs\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, charmap, kernel_size=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.lin = nn.Linear(in_features=128, out_features=128*10) #Channels x Length\n",
    "        self.block1 = ResidualBlock(128)\n",
    "        self.block2 = ResidualBlock(128)\n",
    "        self.block3 = ResidualBlock(128)\n",
    "        self.block4 = ResidualBlock(128)\n",
    "        self.block5 = ResidualBlock(128)\n",
    "        self.conv = nn.Conv1d(in_channels=128, out_channels=len(charmap), kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.lin(inputs).reshape(-1, 128, 10) # for residual blocks\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.conv(x).permute(0, 2, 1)\n",
    "        x = F.softmax(x, dim=2)\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, charmap, kernel_size=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.length_charmap = len(charmap)\n",
    "        self.conv1 = nn.Conv1d(self.length_charmap, 128, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.block1 = ResidualBlock(128)\n",
    "        self.block2 = ResidualBlock(128)\n",
    "        self.block3 = ResidualBlock(128)\n",
    "        self.block4 = ResidualBlock(128)\n",
    "        self.block5 = ResidualBlock(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin = nn.Linear(in_features=128 * 10, out_features=1)\n",
    "    \n",
    "    def forward(self, inputs): #one-hot is input to the discriminator\n",
    "#        x = F.one_hot(inputs, num_classes=self.length_charmap).permute(0, 2, 1).float()\n",
    "        x = inputs.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.flatten(x)\n",
    "        outputs = self.lin(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(4, 128, 10).to(device=device)\n",
    "ResidualBlock(128).to(device=device)(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_inputs = torch.randn(5, 128).to(device=device)\n",
    "gen = Generator(charmap).to(device=device)\n",
    "gen_outputs = gen(gen_inputs).argmax(dim=2)\n",
    "gen_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = next(train)\n",
    "\n",
    "discrim = Discriminator(charmap).to(device=device)\n",
    "inputs = F.one_hot(to_train, num_classes=len(charmap)).to(device=device).float()\n",
    "discrim_outputs = discrim(inputs)\n",
    "print(inputs.shape, discrim_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_outputs_onehot = F.one_hot(gen_outputs, num_classes=len(charmap)).float()\n",
    "discrim(gen_outputs_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(batch_size, 1, 1)\n",
    "    alpha = alpha.expand(real_data.size()).to(device=device)\n",
    "\n",
    "    interpolates = (alpha * real_data + ((1 - alpha) * fake_data)).to(device=device)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    # TODO: Make ConvBackward diffentiable\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device=device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"max_iters: {len(filtered_lines) // 10 // 128}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "lambda_ = 10\n",
    "n_critic_iters_per_generator_iter = 10\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "adam_beta1 = 0.5\n",
    "adam_beta2 = 0.9\n",
    "iterations = 9000\n",
    "\n",
    "one = one = torch.tensor(1, dtype=torch.float).to(device=device)\n",
    "mone = -1 * one\n",
    "\n",
    "netG = Generator(charmap).to(device=device)\n",
    "netD = Discriminator(charmap).to(device=device)\n",
    "\n",
    "# if continue_training:\n",
    "    \n",
    "#     netG = Generator(charmap).to(device)\n",
    "#     netG.load_state_dict(torch.load(\"/home/nvijayakumar/gcp-gan/Checkpoints/netG100004:06:51AM_12-03-20\"))\n",
    "    \n",
    "#     netG = Generator(charmap).to(device)\n",
    "#     netG.load_state_dict(torch.load(\"/home/nvijayakumar/gcp-gan/Checkpoints/netG100004:06:51AM_12-03-20\"))\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    \n",
    "train = dataloader(filtered_lines, batch_size)\n",
    "\n",
    "for iteration in range(1, iterations + 1):\n",
    "    for p in netD.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "        \n",
    "        \n",
    "    for i in range(n_critic_iters_per_generator_iter):\n",
    "        real_inputs_discrete = next(train)\n",
    "        real_data = F.one_hot(real_inputs_discrete, num_classes=len(charmap)).float()\n",
    "        real_data_v = autograd.Variable(real_data)\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        D_real = netD(real_data_v)\n",
    "        D_real = D_real.mean()\n",
    "        # print D_real\n",
    "        # TODO: Waiting for the bug fix from pytorch\n",
    "        D_real.backward(mone)\n",
    "        \n",
    "        noise = torch.randn(batch_size, 128).to(device=device)\n",
    "        with torch.no_grad():\n",
    "            noisev = autograd.Variable(noise)  \n",
    "        fake = autograd.Variable(netG(noisev).data)\n",
    "        inputv = fake\n",
    "        D_fake = netD(inputv)\n",
    "        D_fake = D_fake.mean()\n",
    "        # TODO: Waiting for the bug fix from pytorch\n",
    "        D_fake.backward(one)\n",
    "\n",
    "        \n",
    "        gradient_penalty = calc_gradient_penalty(netD, real_data_v.data, fake.data)\n",
    "        gradient_penalty.backward()\n",
    "        \n",
    "        optimD.step()\n",
    "        netD.zero_grad()\n",
    "    \n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "    netG.zero_grad()\n",
    "\n",
    "    noise = torch.randn(batch_size, 128).to(device=device)\n",
    "    noisev = autograd.Variable(noise)\n",
    "    fake = netG(noisev)\n",
    "    G = netD(fake)\n",
    "    G = G.mean()\n",
    "    G.backward(mone)\n",
    "    G_cost = -G\n",
    "    optimG.step()\n",
    "\n",
    "    if iteration % 500 == 0 or iteration == 1:\n",
    "        print(f\"iterations {iteration}\")\n",
    "        real_translation = translate(real_inputs_discrete[:5].cpu().numpy())\n",
    "        fake_translation = translate(fake[:5].detach().cpu().numpy().argmax(axis=2))\n",
    "        print(f\"\\tFake: {fake_translation}\\n\\tReal: {real_translation}\")\n",
    "        time = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=-5))).strftime(\"%I:%M:%S%p_%m-%d-%y\")\n",
    "        torch.save(netG.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netG{iteration}{time}\")\n",
    "        torch.save(netD.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netD{iteration}{time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG = Generator(charmap).to(device)\n",
    "# netG.load_state_dict(torch.load(\"/home/nvijayakumar/gcp-gan/Checkpoints/netG103:47:35AM_12-03-20\"))\n",
    "\n",
    "# netD = Discriminator(charmap).to(device)\n",
    "# netD.load_state_dict(torch.load(\"/home/nvijayakumar/gcp-gan/Checkpoints/netD103:47:35AM_12-03-20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_noise = torch.randn(10, 128).to(device=device)\n",
    "generated_passwords = netG(latent_noise).argmax(dim=2)\n",
    "translate(generated_passwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "\n",
    "lambda_ = 10\n",
    "LAMBDA = 10\n",
    "n_critic_iters_per_generator_iter = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "adam_beta1 = 0.5\n",
    "adam_beta2 = 0.9\n",
    "iterations = 9000\n",
    "continue_training = True\n",
    "netG_checkpoint = \"/home/nvijayakumar/gcp-gan/Checkpoints/netGnottrash-600011:44:28PM_12-03-20\"\n",
    "netD_checkpoint = \"/home/nvijayakumar/gcp-gan/Checkpoints/netDnottrash-600011:44:28PM_12-03-20\"\n",
    "\n",
    "netG = Generator(charmap).to(device=device)\n",
    "netD = Discriminator(charmap).to(device=device)\n",
    "    \n",
    "train = dataloader(filtered_lines, batch_size)\n",
    "\n",
    "\n",
    "if continue_training:\n",
    "    netG.load_state_dict(torch.load(netG_checkpoint))\n",
    "    netD.load_state_dict(torch.load(netD_checkpoint))\n",
    "    start_iter = int(netG_checkpoint.split(\":\")[0].split(\"-\")[-1][:-2])\n",
    "    for _ in range(start_iter): #look up better way to do this\n",
    "        next(train)\n",
    "        pass\n",
    "    print(f\"Model loaded, starting at {start_iter}...\")\n",
    "else: \n",
    "    start_iter = 1\n",
    "    \n",
    "optimG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(start_iter, iterations + 1):\n",
    "    for p in netD.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "        \n",
    "    for i in range(n_critic_iters_per_generator_iter):\n",
    "        real_inputs_discrete = next(train)\n",
    "        real_data = F.one_hot(real_inputs_discrete, num_classes=len(charmap)).float() #x\n",
    "        latent_variable = torch.randn(batch_size, 128).to(device=device) #z\n",
    "        alpha = torch.rand(batch_size, 1, 1).to(device=device) #epsilon\n",
    "        \n",
    "        fake_data = netG(latent_variable) #x_tilde\n",
    "        \n",
    "        #print(alpha.shape, fake_data.shape, real_data.shape)\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data) #x_hat\n",
    "        interpolates = interpolates.clone().detach().requires_grad_(True) #x_hat\n",
    "        disc_interpolates = netD(interpolates) #D_w(x_hat)\n",
    "        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates, #grad D_w(x_hat)\n",
    "                          grad_outputs=torch.ones(disc_interpolates.size()).to(device=device),\n",
    "                          create_graph=True, retain_graph=True, only_inputs=True)[0] #doesn't populate grad attributes\n",
    "        \n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "        \n",
    "        disc_real = netD(real_data).mean() #D_w(x)\n",
    "        disc_fake = netD(fake_data).mean() #D_w(x_tilde)\n",
    "        \n",
    "        loss = disc_fake - disc_real + gradient_penalty #L\n",
    "        loss.backward()\n",
    "        optimD.step()\n",
    "        netD.zero_grad()\n",
    "    \n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "    netG.zero_grad()\n",
    "    \n",
    "    latent_variable = torch.randn(batch_size, 128).to(device=device) #z\n",
    "    fake_data = netG(latent_variable)    \n",
    "#    noise = torch.randn(batch_size, 128).to(device=device)\n",
    "    G = -netD(fake_data).mean()\n",
    "    G.backward()\n",
    "    optimG.step()\n",
    "\n",
    "    if iteration % 500 == 0 or iteration == 1:\n",
    "        print(f\"iterations {iteration}\")\n",
    "        real_translation = translate(real_inputs_discrete[:10].cpu().numpy())\n",
    "        fake_translation = translate(fake_data[:10].detach().cpu().numpy().argmax(axis=2))\n",
    "        print(f\"\\tFake: {fake_translation}\\n\\tReal: {real_translation}\")\n",
    "        time = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=-5))).strftime(\"%I:%M:%S%p_%m-%d-%y\")\n",
    "        torch.save(netG.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netGfinalrock-{iteration}{time}\")\n",
    "        torch.save(netD.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netDfinalrock-{iteration}{time}\")\n",
    "        \n",
    "# iterations 1\n",
    "# Fake: ['♠ｘﾁこο|خþฒו', 'vXూာင)ףำฝA', '˹ß>ιجאㄓOỳ\\x08', 'J¸ฐXЬに♂Aẻฑ', 'ק唔δฎ̉שþ８Æల', 'үﾝŁ年�淘Ổ中ổй', 'cﾐδκجƒာూㅕą', '我ๅä!နòေåΗΙ', 'Áผศ箮Ϊọעฟ๔E', 'κฺ3ｘ7HѕŁЗ͓']\n",
    "# Real: ['sebasqz|||', 'pipoylove|', 'waffle07||', 'black5678|', '6818597|||', 'ERICILUVU*', 'JUAND|||||', '21801881||', 'farouk93||', 'choiran|||']\n",
    "# iterations 500\n",
    "# Fake: ['uu||ll1uu|', 'uuue||||2e', 'uue|2euuu|', '8uuuuu9|||', 'l11uuuue||', 'l8uuee|22|', 'll11uuue9|', 'uuueee8uu|', 'uuell1uu9|', '8uuu|||29|']\n",
    "# Real: ['sf42007|||', 'kimvan221|', '733390||||', 'kitten7791', 'Schatz4me|', '729tev||||', '2commit|||', '5625381|||', '311y5y9|||', '2262065|||']\n",
    "# iterations 1000\n",
    "# Fake: ['p-$1b0057|', 's150||057|', 'ma|elp-17|', 'san0s15|||', 'pw-15|||||', 'p-11nmm5||', 'mnns1a||||', 'p@a708-1a|', 's150|005||', 'm1nm57||||']\n",
    "# Real: ['2041005|||', 'jaskolka24', 'spindel|||', 'katlen221|', 'maggie1021', 'STAYOUT1||', '0818615011', 'miss0ula!|', '070928945|', 'hondagrand']     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 10\n",
    "LAMBDA = 10\n",
    "n_critic_iters_per_generator_iter = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "adam_beta1 = 0.5\n",
    "adam_beta2 = 0.9\n",
    "iterations = 9000\n",
    "netG = Generator(charmap).to(device=device)\n",
    "netD = Discriminator(charmap).to(device=device)\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    \n",
    "train = dataloader(filtered_lines, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, iterations):\n",
    "    for i in range(n_critic_iters_per_generator_iter):\n",
    "        real_inputs_discrete = next(train)\n",
    "        real_inputs = F.one_hot(real_inputs_discrete, num_classes=len(charmap)).float()\n",
    "        latent_data = torch.randn(batch_size, 128).to(device=device)\n",
    "        fake_inputs = netG(latent_data)\n",
    "        fake_inputs_discrete = fake_inputs.argmax(dim=2)\n",
    "        \n",
    "        disc_real = netD(real_inputs)\n",
    "        disc_fake = netD(fake_inputs)\n",
    "#        disc_cost = torch.mean(disc_fake) - torch.mean(disc_real)\n",
    "#         gen_cost = -torch.mean(disc_fake)\n",
    "        \n",
    "        alpha = torch.rand(batch_size, 1, 1).to(device=device)\n",
    "        differences = fake_inputs - real_inputs\n",
    "        \n",
    "        interpolates = real_inputs + alpha * differences\n",
    "        interpolates.retain_grad()\n",
    "        \n",
    "        temp = netD(interpolates)\n",
    "        temp.mean().backward(retain_graph=True)\n",
    "       \n",
    "        gradients = interpolates.grad\n",
    "\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        \n",
    "        disc_cost = torch.mean(disc_fake) - torch.mean(disc_real) + lambda_ * gradient_penalty\n",
    "        \n",
    "        disc_cost.backward()\n",
    "        optimD.step()\n",
    "        netD.zero_grad()\n",
    "    \n",
    "    latent_data = torch.randn(batch_size, 128).to(device=device)\n",
    "    fake_inputs = netG(latent_data)\n",
    "    disc_fake = netD(fake_inputs)\n",
    "    gen_cost = -torch.mean(disc_fake)\n",
    "    gen_cost.backward()\n",
    "    optimG.step()\n",
    "    netG.zero_grad()\n",
    "    netD.zero_grad()\n",
    "\n",
    "    if iteration % 5 == 0:\n",
    "        print(iteration)\n",
    "        \n",
    "    if iteration % 1000 == 0 or iteration == 1:\n",
    "        print(f\"iterations {iteration}\")\n",
    "        real_translation = translate(real_inputs_discrete[:5].cpu().numpy())\n",
    "        fake_translation = translate(fake_inputs_discrete[:5].cpu().numpy())\n",
    "        print(f\"\\tFake: {fake_translation}\\n\\tReal: {real_translation}\")\n",
    "        torch.save(netG.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netG-{iteration}{time}\")\n",
    "        torch.save(netD.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netD-{iteration}{time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 10\n",
    "LAMBDA = 10\n",
    "n_critic_iters_per_generator_iter = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "adam_beta1 = 0.5\n",
    "adam_beta2 = 0.9\n",
    "iterations = 9000\n",
    "generator = Generator(charmap).to(device=device)\n",
    "discriminator = Discriminator(charmap).to(device=device)\n",
    "\n",
    "optimG = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimD = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    \n",
    "train = dataloader(filtered_lines, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, iterations):\n",
    "    for p in discriminator.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "        \n",
    "    for i in range(n_critic_iters_per_generator_iter):\n",
    "        real_data = next(train)\n",
    "        real_data_onehot = F.one_hot(real_data, num_classes=len(charmap)).float()\n",
    "        \n",
    "        latent_data = torch.randn(batch_size, 128).to(device=device)\n",
    "        fake_data_onehot = generator(latent_data)\n",
    "        #fake_data = fake_data_onehot.argmax(dim=2)\n",
    "        \n",
    "        epsilon = torch.rand(batch_size, 1, 1).to(device=device) #[0,1]\n",
    "        xhat = (epsilon * real_data_onehot + (1 - epsilon) * fake_data_onehot)\n",
    "        xhat.retain_grad()\n",
    "        \n",
    "        xhat_discrim = discriminator(xhat)\n",
    "        torch.mean(xhat_discrim).backward(retain_graph=True)\n",
    "        \n",
    "        gradient_term = lambda_ * torch.square(torch.norm(xhat.grad, dim=1, keepdim=True) - 1.)\n",
    "        loss = (discriminator(fake_data_onehot).mean() - discriminator(real_data_onehot).mean() + gradient_term).mean()\n",
    "    \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimD.step()\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "    for p in discriminator.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "        \n",
    "    latent_data_gen = torch.randn(batch_size, 128).to(device=device)\n",
    "    fake_data_gen = generator(latent_data)\n",
    "    loss_gen = -discriminator(fake_data_gen)\n",
    "    loss_gen.mean().backward()\n",
    "    optimG.step()\n",
    "    generator.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    if iteration % 1000 == 0 or iteration == 1:\n",
    "        print(f\"iterations {iteration}\")\n",
    "        real_translation = translate(real_data[:5].cpu().numpy())\n",
    "        fake_translation = translate(fake_data_gen[:5].argmax(dim=2).cpu().numpy())\n",
    "        print(f\"\\tFake: {fake_translation}\\n\\tReal: {real_translation}\")\n",
    "        time = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=-5))).strftime(\"%I:%M:%S%p_%m-%d-%y\")\n",
    "        torch.save(generator.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netG-{iteration}{time}\")\n",
    "        torch.save(discriminator.state_dict(), f\"/home/nvijayakumar/gcp-gan/Checkpoints/netD-{iteration}{time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = torch.tensor([6., 2, 10], requires_grad=True)\n",
    "z = (x * y).sum()\n",
    "\n",
    "w = z ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd.grad(outputs=z, inputs=x, grad_outputs=torch.ones(z.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
